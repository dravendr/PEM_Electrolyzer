{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import lightgbm\n",
    "import catboost\n",
    "import xgboost\n",
    "#import shap\n",
    "from scipy import interp\n",
    "#import seaborn as sns\n",
    "from catboost import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import r2_score\n",
    "#%matplotlib\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "###########loading data##########\n",
    "fdata=pd.read_csv('database_filled_CD.csv',encoding=\"gbk\")\n",
    "raw_data=fdata.loc[:,[\n",
    "                      'Operating Temperature (℃)',#0\n",
    "                      'Flow Rate (mL min-1)',#1    \n",
    "                      'Active Area (cm2)', #2\n",
    "                      'Ir wt. %',#3\n",
    "                      'Ru wt.%',#4\n",
    "                      'O wt. %',#5\n",
    "                      'I/C in Anode',#6\n",
    "                      'Pt wt. %',#7\n",
    "                      'I/C in Cathode',#8  \n",
    "                      'Anode Precious Metal Loading (mg cm-2 Ir/Ru/Pt/Pd)',#9\n",
    "                      'Cathode Precious Metal Loading (mg cm-2 Pt/Pd)',#10\n",
    "                      'Membrane Thickness (μm)',#11\n",
    "                      'EW'#12\n",
    "                        ]]\n",
    "###########train test splitting##########\n",
    "raw_param=raw_data.iloc[:,0:13]\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CD17=fdata.loc[:,['Current at 1.7 V']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(list1):\n",
    "    total=0\n",
    "    for ele in range(0, len(list1)):\n",
    "        total = total + list1[ele]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(model,param,algorithm_name):\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    result = best_model.predict(X_test)\n",
    "    x_prediction_07=result\n",
    "    y_real_07=y_test[:,0]\n",
    "    x_prediction_07_series=pd.Series(x_prediction_07)\n",
    "    y_real_07_series=pd.Series(y_real_07)\n",
    "    \n",
    "    result_train = best_model.predict(X_train)\n",
    "    x_prediction_07_train=result_train\n",
    "    y_real_07_train=y_train[:,0]\n",
    "    x_prediction_07_series_train=pd.Series(x_prediction_07_train)\n",
    "    y_real_07_series_train=pd.Series(y_real_07_train)\n",
    "    \n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(x_prediction_07_series.corr(y_real_07_series), 5)\n",
    "    error_val= compute_mae_mse_rmse(x_prediction_07,y_real_07)\n",
    "    \n",
    "    corr_ann_train = round(x_prediction_07_series_train.corr(y_real_07_series_train), 5)\n",
    "    error_val_train= compute_mae_mse_rmse(x_prediction_07_train,y_real_07_train)\n",
    "    \n",
    "    print(algorithm_name)\n",
    "    print(best_model.feature_importances_)\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', grid.best_score_)\n",
    "    print(error_val,'TEST R2',error_val[3],'TEST CORR',corr_ann)\n",
    "    print(error_val_train,'TRAIN R2',error_val_train[3],'TRAIN CORR',corr_ann_train)\n",
    "    x_y_x=np.arange(0,3,0.1)\n",
    "    x_y_y=np.arange(0,3,0.1)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_prediction_07,y_real_07,color='red',label=algorithm_name+' Test Set',alpha=0.75)\n",
    "    ax.scatter(x_prediction_07_train,y_real_07_train,color='blue',label=algorithm_name+' Training Set',alpha=0.25,marker=\"^\")\n",
    "    ax.plot(x_y_x,x_y_y)\n",
    "    plt.legend()\n",
    "    plt.xlabel(u\"Predicted_Current_Density (mA cm^-2)@1.7V (vs. RHE)\")\n",
    "    plt.ylabel(u\"Real_Current_Density (mA cm^-2)@1.7V (vs. RHE)\")\n",
    "    plt.savefig('CD17 SEL %s.png' %algorithm_name)\n",
    "    print('finished')\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=51\n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_param, CD17, test_size=.15,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import *\n",
    "import shap\n",
    "def shap_plot(model,param,algorithm_name):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_data.iloc[:,0:13]\n",
    "    SHAP_OUTPUT=CD17\n",
    "    grid = GridSearchCV(model,param_grid=param,cv=5,n_jobs=-100)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        shap_values=shap_values[:,:-1]\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        shap.summary_plot(shap_values, SHAP_INPUT,max_display=100)\n",
    "        global_importances = np.abs(shap_values).mean(0)\n",
    "        print(global_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot_interaction(best_model,algorithm_name,interacted_features):\n",
    "    print(algorithm_name)\n",
    "    SHAP_INPUT=raw_param\n",
    "    SHAP_OUTPUT=CD17\n",
    "    print('train finished')\n",
    "    X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "    y_SHAP=SHAP_OUTPUT.astype(np.float32)\n",
    "\n",
    "    if algorithm_name=='CatBoost':\n",
    "        shap_values = best_model.get_feature_importance(Pool(X_SHAP,y_SHAP), type=\"ShapValues\")\n",
    "        shap_values=shap_values[:,:-1]\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    elif algorithm_name=='Random Forest' or algorithm_name=='Extra Tree'or algorithm_name=='Decision Tree'or algorithm_name=='AdaBoost':\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)        \n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT) \n",
    "        print(shap_values)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    elif algorithm_name=='ANN':\n",
    "        SHAP_INPUT=standardized_data.iloc[:,0:22]\n",
    "        SHAP_OUTPUT=raw_data.iloc[:,22]\n",
    "        X_SHAP=SHAP_INPUT.values.astype(np.float32)\n",
    "        y_SHAP=SHAP_OUTPUT.values.astype(np.float32)\n",
    "        explainer = shap.DeepExplainer(best_model,X_SHAP)\n",
    "        shap_values = explainer.shap_values(X_SHAP) \n",
    "        print(shap_values)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values[0], SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values[0], SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(best_model,SHAP_INPUT)\n",
    "        shap_values = explainer.shap_values(X_SHAP,check_additivity= False)\n",
    "        interaction_values = shap.TreeExplainer(best_model).shap_interaction_values(SHAP_INPUT)\n",
    "        shap.dependence_plot(interacted_features[0], shap_values, SHAP_INPUT,interaction_index= interacted_features[1])\n",
    "        shap.dependence_plot(interacted_features[1], shap_values, SHAP_INPUT,interaction_index= interacted_features[0])\n",
    "from pdpbox import pdp\n",
    "def plot_pdp_interact_ANN(model, df, f_list, cluster_flag=False, nb_clusters=None, lines_flag=False):\n",
    "    \n",
    "    # Create the data that we will plot\n",
    "    inter1 = pdp.pdp_interact(model, df, model_features=df.columns.tolist(), features=f_list,num_grid_points=[20,20])\n",
    "    # plot it\n",
    "    settings = {\n",
    "            'contour_color':  'white',\n",
    "            'font_family': 'Arial',\n",
    "            # matplotlib color map for interact plot\n",
    "            'cmap': 'viridis',\n",
    "            # fill alpha for interact plot\n",
    "            'inter_fill_alpha': 0.8,\n",
    "            # fontsize for interact plot text\n",
    "            'inter_fontsize': 7,\n",
    "        }\n",
    "    pdp.pdp_interact_plot(\n",
    "    pdp_interact_out=inter1, feature_names=f_list, plot_type='contour',figsize=(10,10),x_quantile=True, plot_pdp=True,plot_params=settings)\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.utils import validation\n",
    "def pdp_plot_2d(best_model,f_list):\n",
    "    print('start')\n",
    "    validation.check_is_fitted(estimator=best_model)\n",
    "    my_plots =plot_partial_dependence(best_model, features=[f_list], X=raw_param, percentiles=(0, 1),grid_resolution=100,target=0)\n",
    "def pdp_plot_2d_XG_CAT(best_model,f_list):\n",
    "    print('start')\n",
    "    best_model.dummy_ = \"dummy\"\n",
    "    validation.check_is_fitted(estimator=best_model)\n",
    "    my_plots =plot_partial_dependence(best_model, features=[f_list], X=raw_param, percentiles=(0, 1),grid_resolution=100,target=0)\n",
    "def pdp_plot_2d_ANN(model,f_list):\n",
    "    print('start')\n",
    "    model.dummy_ = \"dummy\"\n",
    "    print(type(model))\n",
    "    validation.check_is_fitted(estimator=model)\n",
    "    my_plots =plot_partial_dependence(model, features=[f_listt], X=raw_input, percentiles=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_feature_list=[['Ir wt. %','Ru wt.%'],[\"Cathode Precious Metal Loading (mg cm-2 Pt/Pd)\",\"I/C in Cathode\"],['Anode Precious Metal Loading (mg cm-2 Ir/Ru/Pt/Pd)','I/C in Anode'],['Membrane Thickness (μm)','EW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost\n",
      "[0.04795676 0.02251863 0.02785666 0.04420057 0.06249793 0.07068411\n",
      " 0.03851399 0.04540989 0.06300753 0.03937636 0.0453732  0.41176417\n",
      " 0.08084019]\n",
      "Best Regressor: {'booster': 'gbtree', 'learning_rate': 0.05, 'max_depth': 11, 'n_estimators': 100, 'reg_alpha': 0.01, 'reg_lambda': 0.001, 'subsample': 0.4} Best Score: 0.7221739074436238\n",
      "(0.16650455973804604, 0.06555765717706104, 0.25604229567995407, 0.8701995123868456) TEST R2 0.8701995123868456 TEST CORR 0.94122\n",
      "(0.08252807716335474, 0.02859646226200141, 0.16910488538774215, 0.932312307637331) TRAIN R2 0.932312307637331 TRAIN CORR 0.97129\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "##########XGBoost gridsearch CV for best hyperparameter##########\n",
    "model_XGBRegressor=xgboost.XGBRegressor(objective='reg:squarederror',random_state=1,verbosity=0)\n",
    "param_xg = {\n",
    "'booster':['gbtree'],\n",
    "'learning_rate':[0.05],\n",
    "'n_estimators':[100],\n",
    "'max_depth':[11],\n",
    "'subsample':[0.4],\n",
    "'reg_alpha':[0.01],\n",
    "'reg_lambda':[0.001]\n",
    "}\n",
    "XG=gridsearch(model_XGBRegressor,param_xg,'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "XGBoost\n",
      "train finished\n",
      "XGBoost\n",
      "train finished\n",
      "XGBoost\n",
      "train finished\n",
      "XGBoost\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "for inter_feature in inter_feature_list:\n",
    "    shap_plot_interaction(XG,algorithm_name=\"XGBoost\",interacted_features=inter_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "start\n",
      "start\n",
      "start\n"
     ]
    }
   ],
   "source": [
    "for inter_feature in inter_feature_list:\n",
    "    pdp_plot_2d_XG_CAT(XG,inter_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
